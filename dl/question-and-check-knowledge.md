# Question & Check knowledge

#### 딥러닝에서 Normalization을 수행하는 이유는?

> Normalization을 하는 이유는 활성화 함수를 수행시키기 위해서 이다. 가령 이미지는 0부터 255의 값을 가지므로, 예를 들어 sigmoid 함수에서 값이 조금만 커져도 output이 1에 매우 쉅게 가까워 질 수 있다. 최적화를 하기 위해 역전파를 수행하여 기울기를 구할 떄, 문제가 될 가능성이 있다. 따라서 이런 문제점을 피하기 위해 모든 값을 0부터 1사이로 두는 것이다.

#### 전이학습과 Downstram Task란?

> 전이학습이란 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법을 가리킵니다. Downstram Task는 프리트레인을 마친 모델을 구조 변경 없이 그대로 사용하거나, 여기에 작은 추가 모듈을 덧붙인 형태로 수행합니다.

#### **Residual Connection이란?**

> 딥러닝 모델은 레이어가 많아지면 학습이 어려운 경향이 있습니다. 모델을 업데이트하기 위한 신호\(그래디언트\)가 전달되는 경로가 길어지기 때문입니다. 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과까지 거둘 수 있습니다.

